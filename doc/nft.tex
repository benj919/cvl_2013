\documentclass[10pt,conference,compsocconf]{IEEEtran}

%\usepackage{times}
%\usepackage{balance}
\usepackage{url}
\usepackage{graphicx}	% For figure environment


\begin{document}
\title{Natural Feature Detection for Augmented Reality}

\author{
  Benjamin Flueck, Hongyi Liu\\
  Supervisors: Amael Delaunoy, Kalin Kolev, Lorenz Meier\\
  Department of Computer Science, ETH Zurich, Switzerland\\
}

\maketitle

\begin{abstract}
The Abstract goes here
\end{abstract}

\section{Introduction}

With the proliferation of powerful mobile phones and their cameras augmented reality (AR) application are now on the verge on becoming ubiquitous. While AR applications existed for along time and can be ported to mobile platforms, many of them rely on predefined patterns. While this is perfectly fine research purposes in a lab it is a considerable inconvenience for every day applications. The goal of this project is therefore to implement a simple augmented reality application that explores the possibility of using arbitrary patterns while considering the limitations of mobile phones.

\section{Method}

Our Application uses a simple combination of a feature detection \& extraction schema supplemented by a KLT tracker. In a first step the application acquires an initial image which should be sufficiently textured. We then do a feature extraction on this image as a starting point for the KLT tracking. As the KLT tracker is limited to  tracking feature points from one frame to the next it is only suitable for tracking already known feature points and not for recovery or detection of the target object. We therefore supply the application with a simple re-detection procedure. This is done by taking a frame and proceeding with a feature detection on it. The features are then directly matched with the ones from the initial image. While the KLT tracker is faster than a full feature detection, extraction, and matching cycle it is prone to loosing track of features and drifting over time. Our application therefore interlaces the KLT tracking with regular re-detection cycles when either the quality or the number of the tracked features drop below certain thresholds. From the matched points we then calculated a homography between between the original image and the the current camera frame, therefore obtaining a estimate for the current position of the original image in the current frame.  In the end this results in a fast and never the less robust tracking of the initial image.

\subsection{Feature Selection and Matching}
In order to be used for our approach the features for the tracking and re-detection have to full-fill certain criteria. They have to be rotation and preferably be scaling invariant and they must be fast to compute. While the rotational invariance is a must the scaling invariant can be substituted with computing the same features on different scales on the image. Due to the relatively small number of features we are operating on the matching of those features can be computed with a straight forward brute force approach as this takes an insignificant time compared to the feature detection and extraction. As we are using the opencv library for this task our selection is limited to what is provided in the library. We have compared the following features detectors for their computational effort:

\begin{itemize}
\item ORB 
\item Briskd
\item Fast
\item Star
\item Dense
\end{itemize}

We then used the following feature descriptors to extract the features detected with the previously mentioned feature detectors:

\begin{itemize}
\item ORB
\item Briskd
\item Brief
\end{itemize}

In the end we decided to use the ORB algorithm for both feature detection and feature descriptors.\\
In order to match these features between different frames we did compare the bruteforce matcher and the FLANN matcher offered by opencv. The bruteforce matcher exactly matches a feature with it's best match by comparing all features with all candidates. The FLANN matcher meanwhile uses locality-sensitive hashing (LSH) in order to approximate the best matches.


\subsection{KLT}

The KLT tracker is a method based on the idea of sparse optical flow. Given a set of initial points it tries to find these points in their vicinity in the subsequent frame. As this is a locally confined method it offers distinct advantages and drawbacks. As the tracker is provided with initial points there is no need to try and find features over the complete image. Assuming small movements between consecutive frames further narrows the possible locations any given point. On the other hand this means that the KLT tracker itself does not discover or retrieve lost tracking points by itself and therefore will suffer from deteriorating performance over time, especially in the presence of occlusions. Furthermore it is more adept to tracking lateral movements and can only follow smaller changes in either scale or rotation. 

\subsection{Homography}

A homography H relates two sets of points on a plane between two different view points. 
\begin{equation}
\left( \begin{array}{c} x \\ y \\ 1 \end{array} \right) = H  \times \left( \begin{array}{c} w*x' \\ w*y' \\ w \end{array} \right)
\end{equation}
$H$ is $3\times3$ matrix representing the change in rotation and translation of the viewpoint between the two sets of points. It can either be used to directly warp an image onto another, in our case the initial image of the target, or it can be decomposed into the aforementioned rotation and translation which can then be used for more elaborate AR applications such as rendering 3D objects on top of the target. 

\section{Implementation}

Our approach is implemented as an android application written mostly in c++ and utilizing the opencv library. The application framework itself is based on the android and opencv tutorial 2 for mixed processing, thereby providing the interface parts for communication between the java and c++ parts of the code. 

\section{Results}
%Our results go here

\section{Lessons learned}
%problems and proposed solutions

\section{Future Work}
%how to improve this mess


\section{Summary}

A short summary of what we have found.

\section*{Acknowledgements}
The authors thank Amael Dalaunoy, Kalin Kolev, and Lorenz Meier for their support throughout the duration of this project. 

\bibliographystyle{IEEEtran}
\bibliography{nft}

\newpage
\appendix
Experiments:\\\\
%Here go the experiments

\appendix
Codebase:\\\\

\appendix
Work distribution:\\\\
%Here goes the work distribution
\begin{tabular}{l l}
Benjamin & Initial Framework \\
& Feature detection \\
& KLT usage \\
& Homography estimation \\
& Initial Visualization \\
& Code for Runtime measurement\\
\\
Hongyi & Performance evaluation \\
& Warped image visualization\\
\\
Both & Inlier ratio for homography \\
& Report\\
\\
\end{tabular} 


\end{document}
